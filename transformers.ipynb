{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_dot_product(query, key, value):\n",
    "    \"\"\"\n",
    "    Dot product of query and key is taken to determine the inital dependance between two embeddings. This score is divided by the square toot of the key vectors which is then passed through a softmax.\n",
    "    The result is then multiplied by the value vector.\n",
    "    \"\"\"\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, q_dim, k_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(input_dim, q_dim)\n",
    "        self.k = nn.Linear(input_dim, k_dim)\n",
    "        self.v = nn.Linear(input_dim, k_dim)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        return attention_dot_product(self.q(query), self.k(key), self.v(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementing multiple attention layers for multiple representation subspaces, enabling the model to focus on different positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, input_dim, q_dim, k_dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionBlock(input_dim, q_dim, k_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(num_heads * k_dim, input_dim)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        return self.linear(torch.cat([x(query, key, value) for x in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positionalEncoding(seq_len, model_dim, device = 'cpu'):\n",
    "    \"\"\"\n",
    "    Positional Embeddings adds a vector to each input embedding to encode the position.\n",
    "    Implemented sinosodial embedding as described by Vaswani et al.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dimensions = torch.arange(model_dim, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** torch.div(dimensions, model_dim, rounding_mode=\"floor\"))\n",
    "\n",
    "    return torch.where(dimensions.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedForward(input_dim = 512, ff_dim = 2048):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, ff_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(ff_dim, input_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residuals(nn.Module):\n",
    "    def __init__(self, sublayer, dimensions, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimensions)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, *tensors):\n",
    "        \"Assuming tensors listed are Query, Key, and Value in the respective order.\"\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim= 512, num_heads= 6, ff_dim= 2048, dropout= 0.1):\n",
    "        super().__init__()\n",
    "        q_dim = k_dim = max(model_dim // num_heads, 1)\n",
    "        self.attention = Residuals(\n",
    "            MultiHeadAttention(num_heads, model_dim, q_dim, k_dim),\n",
    "            dimensions= model_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.feedForward = Residuals(\n",
    "            feedForward(model_dim, ff_dim),\n",
    "            dimensions=model_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, src):\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feedForward(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers= 6, model_dim= 512, num_heads= 8, ff_dim= 2048, dropout= 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(model_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, src):\n",
    "        seq_len, dimensions = src.size(1), src.size(2)\n",
    "        src += positionalEncoding(seq_len, dimensions)\n",
    "        for l in self.layers:\n",
    "            src = l(src)\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim= 512, num_heads= 6, ff_dim= 2048, dropout= 0.1):\n",
    "        super().__init__()\n",
    "        q_dim = k_dim = max(model_dim // num_heads, 1)\n",
    "        self.attention1 = Residuals(\n",
    "            MultiHeadAttention(num_heads, model_dim, q_dim, k_dim),\n",
    "            dimensions= model_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.attention2 = Residuals(\n",
    "            MultiHeadAttention(num_heads, model_dim, q_dim, k_dim),\n",
    "            dimensions= model_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.feedForward = Residuals(\n",
    "            feedForward(model_dim, ff_dim),\n",
    "            dimensions= model_dim,\n",
    "            dropout= dropout\n",
    "        )\n",
    "    \n",
    "    def forward(self, tgt, memory):\n",
    "        tgt = self.attention1(tgt, tgt, tgt)\n",
    "        tgt = self.attention2(tgt, memory, memory)\n",
    "        \n",
    "        return self.feedForward(tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers=6, model_dim= 512, num_heads= 8, ff_dim= 2048, dropout= 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(model_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.linear = nn.Linear(model_dim, model_dim)\n",
    "    \n",
    "    def forward(self, tgt, memory):\n",
    "        seq_len, dimensions = tgt.size(1), tgt.size(2)\n",
    "        tgt += positionalEncoding(seq_len, dimensions)\n",
    "        for l in self.layers:\n",
    "            tgt = l(tgt, memory)\n",
    "        \n",
    "        return torch.softmax(self.linear(tgt), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_enc_layers = 6, num_dec_layers = 6, model_dim = 512, num_heads = 6, ff_dim = 2048, dropout = 0.1, activation = nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers=num_enc_layers, model_dim=model_dim, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "        self.decoder = Decoder(num_layers=num_dec_layers, model_dim=model_dim, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "#Testing out the model\n",
    "\n",
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 32, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf27035b7dc97182e329f4f3264e74024bcb2370fef5d6f059b9761c10d841e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
